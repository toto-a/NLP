{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Antoi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=open(\"names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars=sorted(list(set(\"\".join(words))))\n",
    "stoi={s: i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']=0\n",
    "\n",
    "\n",
    "itos={i:s for s,i in stoi.items()}\n",
    "encode= lambda s : [stoi[c] for c in s] #-->int\n",
    "decode=lambda i :  \"\".join([itos[x] for x in i  ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3])\n",
      "torch.Size([22655, 3])\n",
      "torch.Size([22866, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "block_size=3\n",
    "def build_dataset(words) :\n",
    "    X,Y=[],[]\n",
    "\n",
    "    for w in words:\n",
    "        context=[0] * block_size\n",
    "        for ch in w+ '.' :\n",
    "            ix=stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(\"\".join([itos[i] for i in context]),\" gives \",\"\".join(itos[ix]))\n",
    "            context=context[1:]+ [ix]\n",
    "\n",
    "    X=torch.tensor(X)\n",
    "    Y=torch.tensor(Y)\n",
    "    print(X.shape)\n",
    "    return X,Y\n",
    "\n",
    "import random \n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1=int(0.8*len(words))\n",
    "n2=int(0.9*len(words))\n",
    "\n",
    "Xtr,Ytr=build_dataset(words[:n1])\n",
    "Xdev,Ydev=build_dataset(words[n1:n2])\n",
    "Xte,Yte=build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare gradient\n",
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "vocab_size=27\n",
    "n_emb=10\n",
    "hidden_layer=n_hidden=200\n",
    "\n",
    "C=torch.randn((vocab_size,n_emb))\n",
    "W1=torch.randn((n_emb*block_size,hidden_layer),generator=g) * (5/3)/((n_emb*block_size)**0.5)#*0.2\n",
    "b1=torch.randn(hidden_layer,generator=g)*0.1\n",
    "W2=torch.randn((hidden_layer,vocab_size),generator=g)*0.1\n",
    "b2=torch.randn(vocab_size,generator=g)*0.1\n",
    "\n",
    "bngain=torch.randn(1,n_hidden)*0.1 + 1.0\n",
    "bnbias=torch.randn(1,n_hidden)*.1\n",
    "bnmean_running=torch.zeros(1,n_hidden)\n",
    "bnstd_running=torch.ones(1,n_hidden)\n",
    "parameters=[C,W1,b1,W2,b2,bngain,bnbias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lri=[]\n",
    "lossi=[]\n",
    "stepi=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size # a shorter variable also, for convenience\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7593, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability so that exp() doesn't overflow\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# PyTorch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpreact.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcµat         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "# -----------------\n",
    "\n",
    "Y=F.one_hot(Yb,num_classes=27)\n",
    "dlogprobs=-Y/32\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "\n",
    "dprobs=dlogprobs*1/probs\n",
    "cmp('probs', dprobs, probs)\n",
    "\n",
    "dcounts_sum_inv=(dprobs*counts).sum(1,keepdim=True) \n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "\n",
    "dcounts_sum=dcounts_sum_inv*(-1/counts_sum**2)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "\n",
    "\n",
    "dcounts=torch.ones_like(counts)*dcounts_sum + dprobs*counts_sum_inv\n",
    "cmp('counts', dcounts, counts)\n",
    "\n",
    "dnorm_logits=counts*dcounts\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "\n",
    "dlogit_maxes=(-dnorm_logits).sum(1,keepdim=True)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "\n",
    "dlogits=F.one_hot(logits.max(1).indices,num_classes=logits.shape[1])*dlogit_maxes + dnorm_logits\n",
    "cmp('logits', dlogits, logits)\n",
    "\n",
    "dh=dlogits@W2.T\n",
    "cmp('h', dh, h)\n",
    "\n",
    "dW2=h.T@dlogits\n",
    "cmp('W2', dW2, W2)\n",
    "\n",
    "db2=dlogits.sum(0)\n",
    "cmp('b2', db2, b2)\n",
    "\n",
    "dhpreact=dh*(1-torch.tanh(hpreact)**2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "\n",
    "dbngain=(dhpreact*bnraw).sum(0,keepdim=True)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "\n",
    "dbnbias=dhpreact.sum(0,keepdim=True)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "\n",
    "dbnraw=dhpreact*bngain\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "\n",
    "dbnvar_inv=(dbnraw*bndiff).sum(0,keepdim=True)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "\n",
    "dbnvar=(dbnvar_inv)*(-0.5*((bnvar+1e-5)**-1.5))\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "\n",
    "dbndiff2= 1/(n-1) * torch.ones_like(bndiff2)*dbnvar\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "\n",
    "dbndiff=2*bndiff*dbndiff2 + bnvar_inv*dbnraw\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "\n",
    "dbnmeani=(-1*dbndiff).sum(0)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "\n",
    "dhprebn=dbndiff+ (1/n)*torch.ones_like(hprebn)*dbnmeani\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "\n",
    "dembcat=dhprebn@W1.T\n",
    "cmp('embcµat', dembcat, embcat)\n",
    "\n",
    "dW1=embcat.T@dhprebn\n",
    "cmp('W1', dW1, W1)\n",
    "\n",
    "db1=dhprebn.sum(0)\n",
    "cmp('b1', db1, b1)\n",
    "\n",
    "demb=dembcat.view(emb.shape[0],emb.shape[1], emb.shape[2])\n",
    "cmp('emb', demb, emb)\n",
    "\n",
    "\n",
    "dC=torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]) :\n",
    "    for j in range(Xb.shape[1]) :\n",
    "        ix=Xb[k,j]\n",
    "        dC[ix]+=demb[k,j]\n",
    "\n",
    "\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7629151344299316 diff: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.6104e-04,  6.9559e-04,  9.1139e-04,  3.1884e-04,  7.5230e-04,\n",
       "          8.5073e-04,  7.4304e-04, -2.9837e-02,  1.1258e-03,  1.9946e-03,\n",
       "          6.2782e-04,  1.5654e-03,  1.8000e-04,  6.7390e-03,  4.5210e-04,\n",
       "          2.0327e-03,  8.8493e-04,  1.8751e-03,  6.4239e-04,  3.0904e-04,\n",
       "          3.4170e-04,  5.1870e-04,  1.9513e-03,  1.9469e-03,  5.2962e-04,\n",
       "          4.0189e-04,  1.0853e-03],\n",
       "        [ 9.5695e-04,  1.2250e-03,  7.2955e-04,  1.8292e-03,  1.0490e-03,\n",
       "          6.1403e-04,  1.2267e-03,  2.0571e-03,  3.1434e-03,  2.4961e-04,\n",
       "          7.5121e-04,  8.6155e-04,  3.8467e-03,  2.1346e-04,  1.1377e-03,\n",
       "          2.2949e-03,  4.0153e-04,  1.7926e-03,  5.9910e-04,  2.7025e-04,\n",
       "          4.9008e-04,  7.3389e-04, -3.0666e-02,  7.1439e-04,  8.2570e-04,\n",
       "          1.2596e-03,  1.3925e-03],\n",
       "        [ 9.2975e-04,  5.5157e-04,  5.6078e-04, -3.0073e-02,  6.1974e-04,\n",
       "          7.7691e-04,  7.0684e-04,  1.3975e-03,  2.8441e-03,  3.9557e-04,\n",
       "          6.7194e-04,  9.6842e-04,  6.9202e-03,  5.1018e-04,  1.0257e-03,\n",
       "          1.0629e-03,  1.1125e-03,  1.0395e-03,  8.6303e-04,  3.1533e-04,\n",
       "          8.6202e-04,  1.0530e-03,  7.9403e-04,  5.4731e-04,  7.5040e-04,\n",
       "          1.1193e-03,  1.6747e-03],\n",
       "        [-3.1000e-02,  1.1671e-04,  1.2206e-03,  6.3728e-04,  5.0123e-04,\n",
       "          9.2789e-04,  2.9880e-04,  3.3640e-04,  1.4494e-03,  9.0342e-04,\n",
       "          3.6684e-04,  7.4827e-04,  2.5904e-04,  2.5262e-03,  1.8948e-04,\n",
       "          2.1563e-04,  8.3776e-04,  7.7313e-04,  2.8518e-04,  9.0765e-03,\n",
       "          6.2421e-04,  1.1805e-03,  1.0867e-03,  4.4221e-03,  1.4120e-03,\n",
       "          1.9124e-04,  4.1293e-04],\n",
       "        [ 4.8529e-04,  3.7778e-04,  1.5211e-03,  2.5256e-04,  1.4517e-03,\n",
       "          6.0380e-04,  3.7055e-04,  1.2152e-03,  1.2341e-03,  4.3106e-04,\n",
       "          2.0205e-04,  2.2519e-03,  8.0266e-04,  1.3036e-03,  1.5924e-03,\n",
       "          2.9484e-03,  1.8037e-04,  1.2430e-03,  3.5773e-03,  7.9017e-04,\n",
       "         -3.0892e-02,  1.6419e-03,  1.0454e-03,  1.6239e-03,  4.9381e-04,\n",
       "          3.2410e-04,  2.9279e-03],\n",
       "        [ 1.8339e-04,  1.0242e-03,  4.0005e-04,  1.8216e-03,  1.5998e-03,\n",
       "          1.4932e-03,  2.4942e-03,  2.2334e-04,  4.1470e-04,  3.3834e-03,\n",
       "          2.5512e-03,  1.0568e-03,  1.7999e-04,  2.0941e-03,  2.0479e-03,\n",
       "          7.4217e-04,  7.0058e-04,  2.6657e-03,  6.3415e-04, -3.0362e-02,\n",
       "          2.0141e-04,  6.6598e-04,  6.8221e-04,  1.0062e-03,  8.8743e-04,\n",
       "          1.0512e-03,  1.5712e-04],\n",
       "        [ 2.5238e-04, -3.0167e-02,  1.0528e-03,  1.8038e-04,  5.6266e-04,\n",
       "          1.2364e-03,  6.9827e-04,  2.1272e-03,  1.5687e-04,  1.8805e-03,\n",
       "          9.3460e-04,  8.4326e-04,  4.8642e-04,  2.4545e-03,  2.0512e-03,\n",
       "          1.8298e-03,  8.4047e-04,  1.1874e-03,  7.3599e-04,  1.1315e-03,\n",
       "          4.2810e-04,  1.0916e-03,  1.7322e-03,  3.5226e-03,  4.6087e-04,\n",
       "          5.8803e-04,  1.7007e-03],\n",
       "        [ 9.2975e-04,  5.5157e-04,  5.6078e-04,  1.1768e-03,  6.1974e-04,\n",
       "          7.7691e-04,  7.0684e-04,  1.3975e-03,  2.8441e-03,  3.9557e-04,\n",
       "          6.7194e-04,  9.6842e-04,  6.9202e-03, -3.0740e-02,  1.0257e-03,\n",
       "          1.0629e-03,  1.1125e-03,  1.0395e-03,  8.6303e-04,  3.1533e-04,\n",
       "          8.6202e-04,  1.0530e-03,  7.9403e-04,  5.4731e-04,  7.5040e-04,\n",
       "          1.1193e-03,  1.6747e-03],\n",
       "        [ 6.1205e-04,  5.4779e-04,  6.3468e-04,  2.2013e-04,  1.0025e-03,\n",
       "         -3.0937e-02,  3.9378e-04,  1.5635e-04,  1.2584e-03,  7.6213e-04,\n",
       "          9.0923e-04,  2.1867e-03,  3.1362e-03,  1.3227e-03,  4.4313e-04,\n",
       "          9.7982e-04,  2.6977e-04,  1.4976e-03,  2.2123e-03,  1.7871e-04,\n",
       "          1.4490e-03,  1.0065e-03,  7.4239e-04,  6.6992e-04,  1.6748e-04,\n",
       "          2.5246e-03,  5.6529e-03],\n",
       "        [ 6.3838e-04,  4.7709e-04,  1.1170e-03,  2.3857e-03,  8.5370e-04,\n",
       "          3.5099e-04,  7.0663e-04,  1.5089e-03,  5.0328e-03,  7.9401e-04,\n",
       "          5.8132e-04,  4.6480e-04,  9.0978e-04,  5.9665e-04,  2.1908e-04,\n",
       "          2.8786e-04,  7.5507e-04,  4.2918e-04,  3.1505e-04,  2.6591e-03,\n",
       "          6.5633e-04,  2.7425e-03,  6.2704e-04, -3.0834e-02,  4.3051e-03,\n",
       "          6.7455e-04,  7.4589e-04],\n",
       "        [ 5.3665e-04,  3.2788e-03,  1.2124e-03,  4.6980e-04,  5.6075e-04,\n",
       "          1.4785e-03,  3.7196e-03,  1.0537e-03,  8.6476e-04, -3.0616e-02,\n",
       "          6.2254e-04,  3.8201e-04,  3.9689e-04,  4.3479e-03,  3.3081e-04,\n",
       "          1.8729e-03,  1.7957e-03,  2.3843e-04,  2.6199e-04,  1.7807e-04,\n",
       "          4.2916e-04,  3.0310e-04,  7.0010e-04,  6.2673e-04,  3.4106e-04,\n",
       "          3.8110e-03,  8.0298e-04],\n",
       "        [ 1.8289e-03,  7.0690e-04,  1.0856e-03,  4.8068e-03,  1.9557e-04,\n",
       "          9.6712e-04,  1.8118e-03,  4.3604e-04,  8.4633e-04,  1.9344e-03,\n",
       "          2.5661e-04,  1.1556e-03,  6.9387e-04,  9.6280e-04,  6.0740e-04,\n",
       "          4.5939e-04,  1.2736e-03,  1.0714e-03, -2.9851e-02,  2.4165e-03,\n",
       "          2.5015e-03,  6.6879e-04,  7.4068e-04,  3.2290e-04,  1.7271e-03,\n",
       "          2.7347e-04,  1.0014e-04],\n",
       "        [ 9.2975e-04,  5.5157e-04,  5.6078e-04,  1.1768e-03,  6.1974e-04,\n",
       "          7.7691e-04,  7.0684e-04,  1.3975e-03,  2.8441e-03,  3.9557e-04,\n",
       "          6.7194e-04,  9.6842e-04,  6.9202e-03,  5.1018e-04,  1.0257e-03,\n",
       "          1.0629e-03,  1.1125e-03,  1.0395e-03,  8.6303e-04,  3.1533e-04,\n",
       "          8.6202e-04,  1.0530e-03, -3.0456e-02,  5.4731e-04,  7.5040e-04,\n",
       "          1.1193e-03,  1.6747e-03],\n",
       "        [ 1.2749e-03,  4.5886e-04, -2.9661e-02,  2.6632e-04,  3.4281e-04,\n",
       "          9.8029e-04,  1.7661e-03,  3.6048e-04,  9.8178e-05,  2.8402e-03,\n",
       "          1.1953e-03,  5.6190e-04,  3.1733e-04,  2.7096e-04,  3.1559e-03,\n",
       "          2.6624e-04,  5.6383e-04,  2.2995e-04,  4.9924e-04,  6.4472e-03,\n",
       "          1.7576e-03,  5.6536e-04,  2.0868e-03,  8.3495e-04,  1.2433e-03,\n",
       "          9.5553e-04,  3.2118e-04],\n",
       "        [ 1.9167e-03,  8.9897e-04,  1.2132e-03,  5.1543e-03,  3.7352e-04,\n",
       "          5.1904e-04,  1.3573e-03,  7.8989e-04,  1.4981e-03,  1.2497e-03,\n",
       "          4.0541e-04,  3.5902e-04, -3.0917e-02,  1.7242e-03,  3.0106e-04,\n",
       "          1.9023e-04,  9.4123e-04,  4.1193e-04,  5.7269e-04,  4.9639e-03,\n",
       "          1.4041e-03,  1.1566e-03,  4.2871e-04,  2.5512e-04,  2.1617e-03,\n",
       "          4.8324e-04,  1.8723e-04],\n",
       "        [ 5.9290e-04,  3.0718e-04,  1.6142e-03,  9.4309e-04,  7.8343e-04,\n",
       "          3.8858e-04,  1.0979e-03,  2.3425e-04,  5.2149e-04,  2.2120e-03,\n",
       "          6.6913e-04,  1.0781e-03,  6.2141e-04, -3.0891e-02,  1.4254e-03,\n",
       "          1.7131e-04,  3.7023e-04,  3.8653e-04,  9.4780e-04,  1.1080e-02,\n",
       "          1.9188e-03,  9.4202e-04,  6.0666e-04,  8.1148e-04,  7.5738e-04,\n",
       "          2.5906e-04,  1.5067e-04],\n",
       "        [ 4.1606e-03, -2.9258e-02,  1.5625e-04,  2.5395e-04,  2.8464e-03,\n",
       "          2.7086e-04,  7.4297e-04,  7.6920e-04,  1.3960e-04,  4.9335e-04,\n",
       "          8.3812e-04,  3.6950e-04,  1.7166e-03,  2.2378e-03,  7.9150e-04,\n",
       "          1.7265e-04,  3.0237e-04,  2.0505e-04,  8.0213e-04,  7.4381e-04,\n",
       "          3.3279e-03,  1.8836e-03,  8.3403e-04,  1.8018e-03,  1.0874e-03,\n",
       "          5.1691e-04,  1.7933e-03],\n",
       "        [ 9.2975e-04,  5.5157e-04, -3.0689e-02,  1.1768e-03,  6.1974e-04,\n",
       "          7.7691e-04,  7.0684e-04,  1.3975e-03,  2.8441e-03,  3.9557e-04,\n",
       "          6.7194e-04,  9.6842e-04,  6.9202e-03,  5.1018e-04,  1.0257e-03,\n",
       "          1.0629e-03,  1.1125e-03,  1.0395e-03,  8.6303e-04,  3.1533e-04,\n",
       "          8.6202e-04,  1.0530e-03,  7.9403e-04,  5.4731e-04,  7.5040e-04,\n",
       "          1.1193e-03,  1.6747e-03],\n",
       "        [ 9.2975e-04,  5.5157e-04,  5.6078e-04,  1.1768e-03,  6.1974e-04,\n",
       "          7.7691e-04,  7.0684e-04,  1.3975e-03,  2.8441e-03,  3.9557e-04,\n",
       "         -3.0578e-02,  9.6842e-04,  6.9202e-03,  5.1018e-04,  1.0257e-03,\n",
       "          1.0629e-03,  1.1125e-03,  1.0395e-03,  8.6303e-04,  3.1533e-04,\n",
       "          8.6202e-04,  1.0530e-03,  7.9403e-04,  5.4731e-04,  7.5040e-04,\n",
       "          1.1193e-03,  1.6747e-03],\n",
       "        [ 2.0868e-04,  1.0023e-03,  1.1901e-03,  4.2978e-04,  1.4782e-03,\n",
       "          5.2841e-04,  3.2749e-04,  1.1495e-03,  3.6015e-04,  1.0189e-03,\n",
       "          1.4383e-03,  9.4870e-04,  3.5545e-04,  6.2516e-04,  8.7478e-04,\n",
       "          1.6085e-03,  9.9188e-04,  1.1520e-03,  3.3789e-04, -3.0338e-02,\n",
       "          4.6709e-04,  3.2922e-03,  2.6648e-03,  6.6515e-04,  1.7328e-03,\n",
       "          1.5287e-03,  3.9607e-03],\n",
       "        [ 9.2975e-04,  5.5157e-04,  5.6078e-04,  1.1768e-03,  6.1974e-04,\n",
       "          7.7691e-04,  7.0684e-04,  1.3975e-03,  2.8441e-03,  3.9557e-04,\n",
       "          6.7194e-04,  9.6842e-04,  6.9202e-03, -3.0740e-02,  1.0257e-03,\n",
       "          1.0629e-03,  1.1125e-03,  1.0395e-03,  8.6303e-04,  3.1533e-04,\n",
       "          8.6202e-04,  1.0530e-03,  7.9403e-04,  5.4731e-04,  7.5040e-04,\n",
       "          1.1193e-03,  1.6747e-03],\n",
       "        [ 3.3715e-04,  8.7443e-04,  7.3798e-04,  1.0327e-03,  1.0984e-03,\n",
       "          9.7477e-04,  7.8859e-04,  1.5770e-03,  3.9625e-04, -3.0467e-02,\n",
       "          9.1759e-04,  3.3692e-04,  4.4927e-04,  1.2692e-03,  1.2746e-03,\n",
       "          7.9583e-04,  1.9848e-03,  8.2030e-04,  1.6394e-04,  1.2195e-03,\n",
       "          1.5600e-04,  8.3588e-04,  1.3944e-03,  5.1515e-04,  1.8983e-03,\n",
       "          4.5157e-03,  4.1019e-03],\n",
       "        [ 1.9271e-03,  8.8471e-04,  1.2070e-03,  1.4068e-03,  1.0195e-03,\n",
       "         -3.0265e-02,  1.8837e-03,  4.7478e-04,  1.1117e-03,  1.3139e-03,\n",
       "          5.8711e-04,  1.7375e-03,  1.0978e-03,  3.3680e-04,  2.1133e-03,\n",
       "          1.3374e-03,  3.9744e-04,  1.8696e-03,  2.0476e-03,  2.0324e-03,\n",
       "          9.7449e-04,  1.3851e-03,  1.1853e-03,  5.8707e-04,  4.2592e-04,\n",
       "          7.0527e-04,  2.1573e-04],\n",
       "        [ 2.0713e-03,  1.2907e-03,  2.0551e-03,  3.3629e-04,  1.9216e-04,\n",
       "          9.0222e-04,  3.0215e-04,  2.4855e-04,  1.9264e-03, -3.1041e-02,\n",
       "          2.8372e-04,  5.9856e-04,  1.7641e-03,  8.9884e-04,  1.5423e-04,\n",
       "          2.3621e-03,  2.6810e-03,  2.0616e-04,  3.7400e-04,  1.5992e-03,\n",
       "          6.2151e-03,  6.2793e-04,  6.4911e-04,  1.4080e-03,  6.4458e-04,\n",
       "          5.3758e-04,  7.1234e-04],\n",
       "        [ 5.4491e-04,  6.9444e-04,  1.1017e-03,  4.2911e-03,  8.0912e-04,\n",
       "          1.0165e-03,  1.1296e-03,  4.5510e-04,  1.6434e-03, -2.9888e-02,\n",
       "          1.3387e-03,  1.5819e-03,  2.2129e-04,  1.3877e-03,  3.3514e-04,\n",
       "          2.7396e-04,  5.6243e-04,  2.5689e-04,  9.3049e-04,  1.8602e-03,\n",
       "          1.8173e-03,  9.6898e-04,  6.4340e-04,  1.1496e-03,  3.9667e-03,\n",
       "          6.7636e-04,  2.3087e-04],\n",
       "        [ 1.0599e-03,  1.8644e-03,  4.3266e-04,  6.7319e-05,  1.8272e-03,\n",
       "          4.3832e-04,  8.4155e-04,  5.5506e-04, -3.1171e-02,  3.0077e-03,\n",
       "          1.5284e-03,  3.7688e-04,  1.5769e-03,  3.5691e-03,  1.4574e-03,\n",
       "          1.3039e-04,  9.1461e-04,  3.5506e-04,  4.8767e-04,  5.3033e-04,\n",
       "          1.4795e-03,  2.5309e-03,  9.3038e-04,  2.7170e-03,  5.1012e-04,\n",
       "          1.3912e-03,  5.9113e-04],\n",
       "        [ 7.1529e-04,  1.4167e-03,  4.6907e-04,  7.2066e-04,  1.7677e-03,\n",
       "          5.0629e-04,  1.6875e-03,  1.8968e-03,  1.7575e-04,  1.2032e-03,\n",
       "          1.6162e-03,  4.2014e-04,  4.7306e-04,  4.8713e-04, -2.7124e-02,\n",
       "          6.6259e-04,  9.8841e-04,  4.6149e-04,  3.2820e-04,  1.2065e-03,\n",
       "          5.8525e-04,  2.8938e-03,  1.9110e-03,  5.4964e-04,  8.2305e-04,\n",
       "          1.9204e-03,  1.2378e-03],\n",
       "        [ 7.8067e-04,  1.4951e-03,  4.7026e-04,  1.7190e-03,  2.4823e-03,\n",
       "          3.4188e-04,  2.1649e-03,  1.4913e-03,  8.2151e-04,  8.1140e-04,\n",
       "          4.5598e-04,  6.3340e-04,  1.6558e-03,  3.1985e-04, -2.7017e-02,\n",
       "          5.4499e-04,  4.9270e-04,  1.1119e-03,  5.6242e-04,  1.4771e-03,\n",
       "          5.9138e-04,  1.0716e-03,  1.5354e-03,  2.1838e-04,  1.2777e-03,\n",
       "          1.9462e-03,  5.4377e-04],\n",
       "        [ 9.6073e-04,  5.4724e-04,  9.1601e-04,  5.3949e-04,  9.6609e-04,\n",
       "         -3.0297e-02,  1.4000e-03,  9.3978e-04,  6.3069e-04,  1.1583e-03,\n",
       "          1.1397e-03,  8.1254e-04,  1.3420e-04,  7.0107e-03,  5.9380e-04,\n",
       "          1.5308e-03,  1.7970e-03,  1.1793e-03,  2.1889e-04,  6.2544e-04,\n",
       "          2.3565e-04,  1.6126e-04,  1.0437e-03,  1.1598e-03,  2.5180e-03,\n",
       "          5.8046e-04,  1.4974e-03],\n",
       "        [ 1.4142e-03,  1.7220e-03,  1.7270e-03,  2.5794e-03,  3.4627e-04,\n",
       "          2.5462e-04,  1.1455e-03,  1.2702e-03,  2.1176e-03,  4.2094e-04,\n",
       "          1.6291e-03,  3.8482e-04,  3.5200e-03,  1.7577e-04,  8.8074e-04,\n",
       "          4.5797e-04,  1.3685e-03,  8.7967e-04,  4.6618e-04,  9.0252e-04,\n",
       "         -2.9018e-02,  2.2731e-04,  1.3128e-03,  3.5618e-04,  1.7334e-03,\n",
       "          7.0185e-04,  1.0236e-03],\n",
       "        [ 1.1504e-03,  1.1214e-03,  1.7157e-03,  2.2584e-03,  2.8091e-04,\n",
       "         -3.0654e-02,  1.2975e-03,  1.9581e-03,  4.2502e-03,  3.6999e-04,\n",
       "          4.8853e-04,  1.0785e-03,  3.6041e-03,  2.2463e-04,  2.2946e-04,\n",
       "          1.3472e-03,  6.7993e-04,  8.0699e-04,  4.9017e-04,  4.1640e-04,\n",
       "          1.3501e-03,  8.5153e-04,  7.3332e-04,  5.3267e-04,  2.1368e-03,\n",
       "          5.4297e-04,  7.3805e-04],\n",
       "        [ 1.4863e-03,  1.0361e-03,  1.1679e-03,  6.3817e-04,  1.6336e-03,\n",
       "          9.1993e-04,  1.1574e-03,  2.7200e-03,  1.2241e-03,  7.1364e-04,\n",
       "          1.2741e-03,  5.2301e-04, -3.0902e-02,  2.2686e-03,  2.1110e-03,\n",
       "          3.3173e-03,  1.1999e-03,  1.7277e-03,  4.2130e-04,  4.5302e-04,\n",
       "          3.0802e-04,  2.7955e-04,  1.3388e-03,  4.5349e-04,  1.0072e-03,\n",
       "          4.2653e-04,  1.0954e-03]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits=F.softmax(logits,1)\n",
    "dlogits[range(n),Yb]-=1\n",
    "dlogits/=n\n",
    "\n",
    "cmp('logits',dlogits,logits)\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# -----------------\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn =-1*(bngain * bnvar_inv/n) * (n/(n-1)*bnraw*(dhpreact*bnraw).sum(0) - n*dhpreact + dhpreact.sum(0))\n",
    "\n",
    "# -----------------\n",
    "# dhprebn.shape\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7314\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "#with torch.no_grad():\n",
    "\n",
    "# kick off optimization\n",
    "for i in range(max_steps):\n",
    "\n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmean = hprebn.mean(0, keepdim=True)\n",
    "  bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "  bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "  bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "  hpreact = bngain * bnraw + bnbias\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  # loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "  # manual backprop! #swole_doge_meme\n",
    "  # -----------------\n",
    "  # YOUR CODE HERE :)\n",
    "  grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "  # -----------------\n",
    "\n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p, grad in zip(parameters, grads):\n",
    "    # p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "    p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "  if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(h.view(-1).tolist(),50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibrate the batch norm statistic\n",
    "\n",
    "with torch.no_grad() :\n",
    "    emb=C[Xtr]\n",
    "    embcat=emb.view(emb.shape[0],-1)\n",
    "    hpreact=embcat@W1 + b1\n",
    "    #mesure the mean/std over the entre training set\n",
    "    bnmean=hpreact.mean(0,keepdim=True)\n",
    "    bnstd=hpreact.std(0,keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() #disable gradient tracking\n",
    "def split_loss(split):\n",
    "    dict={'train' : (Xtr,Ytr),\n",
    "         'val' : (Xdev,Ydev),\n",
    "         'test':(Xte,Yte),}\n",
    "    x,y=dict[split]\n",
    "    emb=C[x]\n",
    "    embcat=emb.view(emb.shape[0],-1)\n",
    "    hpreact=embcat@W1 + b1\n",
    "    hpreact=bngain*((hpreact-bnmean)/bnstd) + bnbias\n",
    "    h=torch.tanh(hpreact) ##broadcasting\n",
    "    logits=h@W2 +b2\n",
    "    loss=F.cross_entropy(logits,y)\n",
    "    print(split,loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 2.1870784759521484\n"
     ]
    }
   ],
   "source": [
    "split_loss(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
